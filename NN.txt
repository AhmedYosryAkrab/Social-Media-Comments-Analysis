import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from nltk.stem import PorterStemmer
from sklearn.model_selection import train_test_split
import re
import numpy as np
from sklearn import preprocessing
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import metrics
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from tensorflow import keras
from keras import Sequential
from keras.layers import Dense
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import f1_score

!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

downloaded = drive.CreateFile({'id':'1kri6Ae0MqbHPsToPLoT6ZngmwGTeUtnS'})
downloaded.GetContentFile('train.csv')

df= pd.read_csv('train.csv',encoding='latin-1')
df.head()

downloaded = drive.CreateFile({'id':'1W3kHCE8ER_8KnP8pf9ap78xi9niPGoFC'})
downloaded.GetContentFile('stopwords.txt')

stopwords = open("stopwords.txt",'r').read()
stopwords = stopwords.split('\n')
stopwords

def stopwords_remove(text):
    text        = text.lower()
    words       = text.split(' ')
    clean_words = []
    for word in words:
        if not word in stopwords:
            clean_words.append(word)
    return ' '.join(clean_words)

def denoise(text):
    return re.sub("[^a-zA-Z ]*","",text).strip()

ps = PorterStemmer()

def stemm(text):
    clean_text = ""
    for word in text.split():
        clean_text +=" "+ps.stem(word)
    return clean_text

def clean(text):
    text = stopwords_remove(text)
    text = denoise(text)
    text = stemm(text)
    return text

df['clean_text'] = df.tweet.apply(clean)
df.head()

lb_make = LabelEncoder()

X=df["clean_text"] = lb_make.fit_transform(df["clean_text"])

X

Y=df["label"] = lb_make.fit_transform(df["label"])

Y

sc=StandardScaler(copy=True, with_mean=False, with_std=True) 

x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)

model = Sequential()
model.add(Dense(4, activation='relu', kernel_initializer='random_normal', input_dim=1))
model.add(Dense(4, activation='relu', kernel_initializer='random_normal'))
model.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

history=model.fit(x_train,y_train,batch_size=10, epochs=50)

test_loss, test_acc = model.evaluate(x_test,y_test, verbose=2)

test_loss

test_acc*100

predictions = model.predict(x_test)

predictions

plt.plot(history.history['acc'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

# Plot training & validation loss values
plt.plot(history.history['loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

